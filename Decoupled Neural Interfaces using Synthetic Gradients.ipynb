{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of Decoupled Neural Interfaces using Synthetic Gradients\n",
    "## The following is an implementation of the following paper \n",
    "## https://arxiv.org/abs/1608.05343\n",
    "## This is a simplistic implementation to judge the performance of the technique over simpler architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:99 Loss:898.862526519\n",
      "Iter:199 Loss:236.238081927\n",
      "Iter:299 Loss:31.384377941\n",
      "Iter:399 Loss:12.3799158643\n",
      "Iter:499 Loss:6.5984529345\n",
      "Iter:599 Loss:4.52426686187\n",
      "Iter:699 Loss:3.37290974735\n",
      "Iter:799 Loss:2.67397835444\n",
      "Iter:899 Loss:2.27340321919\n",
      "Iter:999 Loss:1.9615651384\n"
     ]
    }
   ],
   "source": [
    "# Simple Neural Network with backprop\n",
    "\n",
    "# Importig libraries\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "# Generating dataset for biary addition\n",
    "def generate_dataset(output_dim = 8,num_examples=1000):\n",
    "    def int2vec(x,dim=output_dim):\n",
    "        out = np.zeros(dim)\n",
    "        binrep = np.array(list(np.binary_repr(x))).astype('int')\n",
    "        out[-len(binrep):] = binrep\n",
    "        return out\n",
    "\n",
    "    x_left_int = (np.random.rand(num_examples) * 2**(output_dim - 1)).astype('int')\n",
    "    x_right_int = (np.random.rand(num_examples) * 2**(output_dim - 1)).astype('int')\n",
    "    y_int = x_left_int + x_right_int\n",
    "\n",
    "    x = list()\n",
    "    for i in range(len(x_left_int)):\n",
    "        x.append(np.concatenate((int2vec(x_left_int[i]),int2vec(x_right_int[i]))))\n",
    "\n",
    "    y = list()\n",
    "    for i in range(len(y_int)):\n",
    "        y.append(int2vec(y_int[i]))\n",
    "\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    return (x,y)\n",
    "    \n",
    "np.random.seed(1)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "num_examples = 1000\n",
    "output_dim = 12\n",
    "iterations = 1000\n",
    "\n",
    "x,y = generate_dataset(num_examples=num_examples, output_dim = output_dim)\n",
    "\n",
    "batch_size = 10\n",
    "alpha = 0.1\n",
    "\n",
    "input_dim = len(x[0])\n",
    "layer_1_dim = 128\n",
    "layer_2_dim = 64\n",
    "output_dim = len(y[0])\n",
    "\n",
    "weights_0_1 = (np.random.randn(input_dim,layer_1_dim) * 0.2) - 0.1\n",
    "weights_1_2 = (np.random.randn(layer_1_dim,layer_2_dim) * 0.2) - 0.1\n",
    "weights_2_3 = (np.random.randn(layer_2_dim,output_dim) * 0.2) - 0.1\n",
    "\n",
    "\n",
    "for iter in range(iterations):\n",
    "    error = 0\n",
    "\n",
    "    for batch_i in range(int(len(x) / batch_size)):\n",
    "        batch_x = x[(batch_i * batch_size):(batch_i+1)*batch_size]\n",
    "        batch_y = y[(batch_i * batch_size):(batch_i+1)*batch_size]    \n",
    "\n",
    "        layer_0 = batch_x\n",
    "        layer_1 = sigmoid(layer_0.dot(weights_0_1))\n",
    "        layer_2 = sigmoid(layer_1.dot(weights_1_2))\n",
    "        layer_3 = sigmoid(layer_2.dot(weights_2_3))\n",
    "\n",
    "        layer_3_delta = (layer_3 - batch_y) * layer_3  * (1 - layer_3)\n",
    "        layer_2_delta = layer_3_delta.dot(weights_2_3.T) * layer_2 * (1 - layer_2)\n",
    "        layer_1_delta = layer_2_delta.dot(weights_1_2.T) * layer_1 * (1 - layer_1)\n",
    "\n",
    "        weights_0_1 -= layer_0.T.dot(layer_1_delta) * alpha\n",
    "        weights_1_2 -= layer_1.T.dot(layer_2_delta) * alpha\n",
    "        weights_2_3 -= layer_2.T.dot(layer_3_delta) * alpha\n",
    "\n",
    "        error += (np.sum(np.abs(layer_3_delta)))\n",
    "\n",
    "    sys.stdout.write(\"\\rIter:\" + str(iter) + \" Loss:\" + str(error))\n",
    "    if(iter % 100 == 99):\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:9999 Loss:324.912688062 Synthetic Loss:1633.27356925\n",
      "Iter:19999 Loss:288.362494883 Synthetic Loss:1551.51824491\n",
      "Iter:29999 Loss:308.729520313 Synthetic Loss:1673.95826899\n",
      "Iter:39999 Loss:306.222822868 Synthetic Loss:1715.61896236\n",
      "Iter:49999 Loss:282.584591549 Synthetic Loss:1638.65900821\n",
      "Iter:59999 Loss:296.197338217 Synthetic Loss:1623.30532484\n",
      "Iter:69999 Loss:302.873070742 Synthetic Loss:1786.50582267\n",
      "Iter:79999 Loss:308.701329144 Synthetic Loss:1861.99697025\n",
      "Iter:89999 Loss:319.27358406 Synthetic Loss:2025.46988944\n",
      "Iter:99999 Loss:309.846240006 Synthetic Loss:1963.62973659\n"
     ]
    }
   ],
   "source": [
    "# Synthetic Gradient Network\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "def generate_dataset(output_dim = 8,num_examples=1000):\n",
    "    def int2vec(x,dim=output_dim):\n",
    "        out = np.zeros(dim)\n",
    "        binrep = np.array(list(np.binary_repr(x))).astype('int')\n",
    "        out[-len(binrep):] = binrep\n",
    "        return out\n",
    "\n",
    "    x_left_int = (np.random.rand(num_examples) * 2**(output_dim - 1)).astype('int')\n",
    "    x_right_int = (np.random.rand(num_examples) * 2**(output_dim - 1)).astype('int')\n",
    "    y_int = x_left_int + x_right_int\n",
    "\n",
    "    x = list()\n",
    "    for i in range(len(x_left_int)):\n",
    "        x.append(np.concatenate((int2vec(x_left_int[i]),int2vec(x_right_int[i]))))\n",
    "\n",
    "    y = list()\n",
    "    for i in range(len(y_int)):\n",
    "        y.append(int2vec(y_int[i]))\n",
    "\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    return (x,y)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_out2deriv(out):\n",
    "    return out * (1 - out)\n",
    "\n",
    "class DNI(object):\n",
    "    \n",
    "    def __init__(self,input_dim, output_dim,nonlin,nonlin_deriv,alpha = 0.1):\n",
    "        \n",
    "        self.weights = (np.random.randn(input_dim, output_dim) * 2) - 1\n",
    "        self.bias = (np.random.randn(output_dim) * 2) - 1\n",
    "        \n",
    "        self.weights_0_1_synthetic_grads = (np.random.randn(output_dim,output_dim) * .0) - .0\n",
    "        self.bias_0_1_synthetic_grads = (np.random.randn(output_dim) * .0) - .0\n",
    "    \n",
    "        self.nonlin = nonlin\n",
    "        self.nonlin_deriv = nonlin_deriv\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    def forward_and_synthetic_update(self,input,update=True):\n",
    "        \n",
    "        self.input = input\n",
    "        self.output = self.nonlin(self.input.dot(self.weights)  + self.bias)\n",
    "        \n",
    "        if(not update):\n",
    "            return self.output\n",
    "        else:\n",
    "            self.synthetic_gradient = (self.output.dot(self.weights_0_1_synthetic_grads) + self.bias_0_1_synthetic_grads)\n",
    "            self.weight_synthetic_gradient = self.synthetic_gradient * self.nonlin_deriv(self.output)\n",
    "        \n",
    "            self.weights -= self.input.T.dot(self.weight_synthetic_gradient) * self.alpha\n",
    "            self.bias -= np.average(self.weight_synthetic_gradient,axis=0) * self.alpha\n",
    "        \n",
    "        return self.weight_synthetic_gradient.dot(self.weights.T), self.output\n",
    "    \n",
    "    def normal_update(self,true_gradient):\n",
    "        grad = true_gradient * self.nonlin_deriv(self.output)\n",
    "        \n",
    "        self.weights -= self.input.T.dot(grad) * self.alpha\n",
    "        self.bias -= np.average(grad,axis=0) * self.alpha\n",
    "        \n",
    "        return grad.dot(self.weights.T)\n",
    "    \n",
    "    def update_synthetic_weights(self,true_gradient):\n",
    "        self.synthetic_gradient_delta = (self.synthetic_gradient - true_gradient)\n",
    "        self.weights_0_1_synthetic_grads -= self.output.T.dot(self.synthetic_gradient_delta) * self.alpha\n",
    "        self.bias_0_1_synthetic_grads -= np.average(self.synthetic_gradient_delta,axis=0) * self.alpha\n",
    "        \n",
    "np.random.seed(1)\n",
    "\n",
    "num_examples = 100\n",
    "output_dim = 8\n",
    "iterations = 100000\n",
    "\n",
    "x,y = generate_dataset(num_examples=num_examples, output_dim = output_dim)\n",
    "\n",
    "batch_size = 10\n",
    "alpha = 0.01\n",
    "\n",
    "input_dim = len(x[0])\n",
    "layer_1_dim = 64\n",
    "layer_2_dim = 32\n",
    "output_dim = len(y[0])\n",
    "\n",
    "layer_1 = DNI(input_dim,layer_1_dim,sigmoid,sigmoid_out2deriv,alpha)\n",
    "layer_2 = DNI(layer_1_dim,layer_2_dim,sigmoid,sigmoid_out2deriv,alpha)\n",
    "layer_3 = DNI(layer_2_dim, output_dim,sigmoid, sigmoid_out2deriv,alpha)\n",
    "\n",
    "for iter in range(iterations):\n",
    "    error = 0\n",
    "    synthetic_error = 0\n",
    "    \n",
    "    for batch_i in range(int(len(x) / batch_size)):\n",
    "        batch_x = x[(batch_i * batch_size):(batch_i+1)*batch_size]\n",
    "        batch_y = y[(batch_i * batch_size):(batch_i+1)*batch_size]  \n",
    "        \n",
    "        _, layer_1_out = layer_1.forward_and_synthetic_update(batch_x)\n",
    "        layer_1_delta, layer_2_out = layer_2.forward_and_synthetic_update(layer_1_out)\n",
    "        layer_3_out = layer_3.forward_and_synthetic_update(layer_2_out,False)\n",
    "\n",
    "        layer_3_delta = layer_3_out - batch_y\n",
    "        layer_2_delta = layer_3.normal_update(layer_3_delta)\n",
    "        layer_2.update_synthetic_weights(layer_2_delta)\n",
    "        layer_1.update_synthetic_weights(layer_1_delta)\n",
    "        \n",
    "        error += (np.sum(np.abs(layer_3_delta)))\n",
    "        synthetic_error += (np.sum(np.abs(layer_2_delta - layer_2.synthetic_gradient)))\n",
    "    if(iter % 100 == 99):\n",
    "        sys.stdout.write(\"\\rIter:\" + str(iter) + \" Loss:\" + str(error) + \" Synthetic Loss:\" + str(synthetic_error))\n",
    "    if(iter % 10000 == 9999):\n",
    "        print(\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
